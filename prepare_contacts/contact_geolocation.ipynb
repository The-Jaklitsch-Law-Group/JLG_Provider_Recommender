{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2bf952",
   "metadata": {},
   "source": [
    "# Geolocating Filevine Contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0093565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "from geopy.geocoders import Nominatim\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c0543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load provider data\n",
    "DATA_PATH = './data/cleaned_outbound_referrals.parquet'\n",
    "df = pd.read_parquet(DATA_PATH)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91690bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019aca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'Full Address' not in df.columns:\n",
    "#     df['Full Address'] = (\n",
    "#         df['Street'].fillna('') + ', '\n",
    "#         + df['City'].fillna('') + ', '\n",
    "#         + df['State'].fillna('') + ' '\n",
    "#         + df['Zip'].fillna('')\n",
    "#     )\n",
    "#     df['Full Address'] = df['Full Address'].str.replace(r',\\s*,', ',', regex=True).str.replace(r',\\s*$', '', regex=True)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary:\n",
    "# - Free-first, DMV-focused async geocoding cascade: US Census (structured) → US Census (oneline) → Nominatim (DMV-bounded).\n",
    "# - Aggressive fallbacks: Full Address → Street+City+State → Street+Zip → Street-only.\n",
    "# - Adds: Latitude, Longitude, GeocodeSource, GeocodeQuality, ConfidenceScore (normalized 0–1), GeocodeStatus.\n",
    "# - Enforces Nominatim etiquette (≥1 req/sec) and filters out results outside the DMV bounding box.\n",
    "\n",
    "import asyncio\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "\n",
    "import pandas as pd\n",
    "import httpx\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "# US Census endpoints (free)\n",
    "CENSUS_STRUCT_URL = \"https://geocoding.geo.census.gov/geocoder/locations/address\"\n",
    "CENSUS_ONE_URL    = \"https://geocoding.geo.census.gov/geocoder/locations/onelineaddress\"\n",
    "CENSUS_PARAMS_BASE = {\n",
    "    \"benchmark\": \"Public_AR_Current\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "# Nominatim (free; follow usage policy)\n",
    "NOMINATIM_URL = \"https://nominatim.openstreetmap.org/search\"\n",
    "NOMINATIM_USER_AGENT = \"my-firm-provider-geocoder (benjamin@jaklitschlawgroup.com)\"  # <-- put your contact\n",
    "NOMINATIM_MIN_INTERVAL = 1.2  # ≥1 req/sec (safer than 1.0)\n",
    "\n",
    "# DMV bounding box (approx): SW & NE corners (lat, lng)\n",
    "DMV_BBOX = {\n",
    "    \"sw_lat\": 38.40, \"sw_lng\": -77.60,\n",
    "    \"ne_lat\": 39.40, \"ne_lng\": -76.00\n",
    "}\n",
    "\n",
    "# Concurrency controls\n",
    "MAX_CONCURRENCY = 6  # tasks can be scheduled; per-source limiter still enforces pacing\n",
    "\n",
    "# Column mapping: adapt to your DataFrame schema\n",
    "DEFAULT_COLS = {\n",
    "    \"street\": \"Street\",              # e.g., \"Address 1 Line 1\" if that's your field\n",
    "    \"city\":   \"City\",\n",
    "    \"state\":  \"State\",\n",
    "    \"zip\":    \"Zip\",\n",
    "    \"full\":   \"Full Address\"\n",
    "}\n",
    "\n",
    "# -------------------- HELPERS --------------------\n",
    "class AsyncRateLimiter:\n",
    "    \"\"\"Enforce a minimum interval between calls across all coroutines.\"\"\"\n",
    "    def __init__(self, min_interval_seconds: float = 1.0):\n",
    "        self.min_interval = float(min_interval_seconds)\n",
    "        self._lock = asyncio.Lock()\n",
    "        self._last = 0.0\n",
    "    async def wait(self):\n",
    "        async with self._lock:\n",
    "            now = time.monotonic()\n",
    "            wait_for = self.min_interval - (now - self._last)\n",
    "            if wait_for > 0:\n",
    "                await asyncio.sleep(wait_for)\n",
    "            self._last = time.monotonic()\n",
    "\n",
    "def _normalize_columns(df: pd.DataFrame, cols: Dict[str, str]):\n",
    "    \"\"\"Ensure address columns exist, are strings, and build a clean Full Address when missing.\"\"\"\n",
    "    for key in (\"street\", \"city\", \"state\", \"zip\"):\n",
    "        c = cols[key]\n",
    "        if c not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {c!r} mapped from {key!r}\")\n",
    "        df[c] = df[c].astype(str).fillna(\"\").str.strip()\n",
    "\n",
    "    # Build Full Address if not present\n",
    "    if cols[\"full\"] not in df.columns:\n",
    "        df[cols[\"full\"]] = (\n",
    "            df[cols[\"street\"]] + \", \" +\n",
    "            df[cols[\"city\"]]   + \", \" +\n",
    "            df[cols[\"state\"]]  + \" \" +\n",
    "            df[cols[\"zip\"]]\n",
    "        )\n",
    "    # Clean punctuation/spacing\n",
    "    df[cols[\"full\"]] = (\n",
    "        df[cols[\"full\"]]\n",
    "          .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "          .str.replace(r\",\\s*,\", \",\", regex=True)\n",
    "          .str.replace(r\",\\s*$\", \"\", regex=True)\n",
    "          .str.strip()\n",
    "    )\n",
    "\n",
    "    # Ensure output columns exist\n",
    "    for c in (\"Latitude\", \"Longitude\", \"GeocodeSource\", \"GeocodeQuality\", \"ConfidenceScore\", \"GeocodeStatus\"):\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "def _in_dmv_bbox(lat: float, lng: float) -> bool:\n",
    "    return (DMV_BBOX[\"sw_lat\"] <= lat <= DMV_BBOX[\"ne_lat\"] and\n",
    "            DMV_BBOX[\"sw_lng\"] <= lng <= DMV_BBOX[\"ne_lng\"])\n",
    "\n",
    "# Quality helpers\n",
    "def _score_census() -> Tuple[str, float]:\n",
    "    # Census often returns a good street/parcel-level point (TIGER-based)\n",
    "    return (\"census:match\", 0.85)  # normalized to 0–1\n",
    "\n",
    "def _score_nominatim(n_type: Optional[str], importance: Optional[float]) -> Tuple[str, float]:\n",
    "    # Favor house/building/address types; scale by \"importance\".\n",
    "    t = (n_type or \"\").lower()\n",
    "    base = 0.0\n",
    "    if t in {\"house\", \"building\", \"address\"}:\n",
    "        base = 0.75\n",
    "    elif t in {\"residential\", \"road\"}:\n",
    "        base = 0.55\n",
    "    else:\n",
    "        base = 0.45\n",
    "    imp = float(importance) if importance is not None else 0.3\n",
    "    # Blend and clamp to [0,1]\n",
    "    score = max(0.0, min(1.0, 0.5 * base + 0.5 * min(imp, 1.0)))\n",
    "    return (f\"nominatim:{t or 'unknown'}\", score)\n",
    "\n",
    "# -------------------- SOURCE CALLS --------------------\n",
    "async def _census_structured(\n",
    "    client: httpx.AsyncClient, street: str, city: str, state: str, zip_code: str\n",
    ") -> Optional[Dict]:\n",
    "    if not street:\n",
    "        return None\n",
    "    params = dict(CENSUS_PARAMS_BASE)\n",
    "    params.update({\"street\": street, \"city\": city, \"state\": state, \"zip\": zip_code})\n",
    "    try:\n",
    "        r = await client.get(CENSUS_STRUCT_URL, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        data = r.json()\n",
    "        matches = data.get(\"result\", {}).get(\"addressMatches\", [])\n",
    "        if not matches:\n",
    "            return None\n",
    "        # Take best match\n",
    "        m0 = matches[0]\n",
    "        coords = m0.get(\"coordinates\", {})\n",
    "        lat = float(coords.get(\"y\"))\n",
    "        lng = float(coords.get(\"x\"))\n",
    "        quality, score = _score_census()\n",
    "        return {\"lat\": lat, \"lng\": lng, \"source\": \"census_struct\", \"quality\": quality, \"score\": score}\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "async def _census_oneline(client: httpx.AsyncClient, one_line: str) -> Optional[Dict]:\n",
    "    if not one_line:\n",
    "        return None\n",
    "    params = dict(CENSUS_PARAMS_BASE)\n",
    "    params.update({\"address\": one_line})\n",
    "    try:\n",
    "        r = await client.get(CENSUS_ONE_URL, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        data = r.json()\n",
    "        matches = data.get(\"result\", {}).get(\"addressMatches\", [])\n",
    "        if not matches:\n",
    "            return None\n",
    "        m0 = matches[0]\n",
    "        coords = m0.get(\"coordinates\", {})\n",
    "        lat = float(coords.get(\"y\"))\n",
    "        lng = float(coords.get(\"x\"))\n",
    "        quality, score = _score_census()\n",
    "        return {\"lat\": lat, \"lng\": lng, \"source\": \"census_oneline\", \"quality\": quality, \"score\": score}\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "class _NominatimWrapper:\n",
    "    \"\"\"Nominatim with a global rate limiter and DMV bounding.\"\"\"\n",
    "    def __init__(self, min_interval=NOMINATIM_MIN_INTERVAL, user_agent=NOMINATIM_USER_AGENT):\n",
    "        self.limiter = AsyncRateLimiter(min_interval)\n",
    "        self.headers = {\"User-Agent\": user_agent, \"Accept\": \"application/json\"}\n",
    "\n",
    "    async def search(self, client: httpx.AsyncClient, q: str) -> Optional[Dict]:\n",
    "        if not q:\n",
    "            return None\n",
    "        params = {\n",
    "            \"q\": q,\n",
    "            \"format\": \"json\",\n",
    "            \"limit\": 1,\n",
    "            \"addressdetails\": 0,\n",
    "            \"countrycodes\": \"us\",\n",
    "            # DMV bounding box (lng,lat order inside viewbox spec)\n",
    "            \"viewbox\": f'{DMV_BBOX[\"sw_lng\"]},{DMV_BBOX[\"ne_lat\"]},{DMV_BBOX[\"ne_lng\"]},{DMV_BBOX[\"sw_lat\"]}',\n",
    "            \"bounded\": 1\n",
    "        }\n",
    "        await self.limiter.wait()\n",
    "        try:\n",
    "            r = await client.get(NOMINATIM_URL, params=params, headers=self.headers, timeout=30)\n",
    "            if r.status_code != 200:\n",
    "                return None\n",
    "            arr = r.json()\n",
    "            if not arr:\n",
    "                return None\n",
    "            item = arr[0]\n",
    "            lat = float(item.get(\"lat\"))\n",
    "            lng = float(item.get(\"lon\"))\n",
    "            n_type = item.get(\"type\")\n",
    "            importance = item.get(\"importance\")\n",
    "            quality, score = _score_nominatim(n_type, importance)\n",
    "            return {\"lat\": lat, \"lng\": lng, \"source\": \"nominatim\", \"quality\": quality, \"score\": score}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "NOMI = _NominatimWrapper()\n",
    "\n",
    "# -------------------- PER-ROW GEOCODING --------------------\n",
    "async def _geocode_one_row(\n",
    "    idx: int, df: pd.DataFrame, client: httpx.AsyncClient, cols: Dict[str, str], cache: Dict[str, Optional[Dict]]\n",
    "):\n",
    "    # If already has coordinates, set status and return\n",
    "    if pd.notna(df.at[idx, \"Latitude\"]) and pd.notna(df.at[idx, \"Longitude\"]):\n",
    "        if pd.isna(df.at[idx, \"GeocodeStatus\"]):\n",
    "            df.at[idx, \"GeocodeStatus\"] = \"skipped_has_coords\"\n",
    "        return\n",
    "\n",
    "    street = str(df.at[idx, cols[\"street\"]]).strip()\n",
    "    city   = str(df.at[idx, cols[\"city\"]]).strip()\n",
    "    state  = str(df.at[idx, cols[\"state\"]]).strip()\n",
    "    zipc   = str(df.at[idx, cols[\"zip\"]]).strip()\n",
    "    full   = str(df.at[idx, cols[\"full\"]]).strip()\n",
    "\n",
    "    # Candidate queries in descending specificity\n",
    "    queries = [\n",
    "        (\"full_struct\", (street, city, state, zipc)),                 # Census structured\n",
    "        (\"full_oneline\", (full,)),                                    # Census oneline\n",
    "        (\"street_city_state\", (f\"{street}, {city}, {state}\",)),       # Nominatim\n",
    "        (\"street_zip\", (f\"{street}, {zipc}\",)),                       # Nominatim\n",
    "        (\"street_only\", (street,)),                                   # Nominatim\n",
    "    ]\n",
    "\n",
    "    best = None\n",
    "\n",
    "    # 1) Census structured\n",
    "    key = f\"CENSUS_STRUCT::{street}|{city}|{state}|{zipc}\"\n",
    "    if key in cache:\n",
    "        res = cache[key]\n",
    "    else:\n",
    "        res = await _census_structured(client, street, city, state, zipc)\n",
    "        cache[key] = res\n",
    "    if res and _in_dmv_bbox(res[\"lat\"], res[\"lng\"]):\n",
    "        best = res\n",
    "\n",
    "    # 2) Census oneline (if needed)\n",
    "    if best is None and full:\n",
    "        key = f\"CENSUS_ONE::{full}\"\n",
    "        if key in cache:\n",
    "            res = cache[key]\n",
    "        else:\n",
    "            res = await _census_oneline(client, full)\n",
    "            cache[key] = res\n",
    "        if res and _in_dmv_bbox(res[\"lat\"], res[\"lng\"]):\n",
    "            best = res\n",
    "\n",
    "    # 3) Nominatim fallbacks with DMV bounding\n",
    "    if best is None:\n",
    "        for label, args in queries[2:]:\n",
    "            q = args[0]\n",
    "            k = f\"NOMI::{label}::{q}\"\n",
    "            if k in cache:\n",
    "                res = cache[k]\n",
    "            else:\n",
    "                res = await NOMI.search(client, q)\n",
    "                cache[k] = res\n",
    "            if res and _in_dmv_bbox(res[\"lat\"], res[\"lng\"]):\n",
    "                # Take the first DMV-in result with the highest score encountered\n",
    "                if best is None or res[\"score\"] > best[\"score\"]:\n",
    "                    best = res\n",
    "\n",
    "    # Write back\n",
    "    if best is not None:\n",
    "        df.at[idx, \"Latitude\"]        = best[\"lat\"]\n",
    "        df.at[idx, \"Longitude\"]       = best[\"lng\"]\n",
    "        df.at[idx, \"GeocodeSource\"]   = best[\"source\"]\n",
    "        df.at[idx, \"GeocodeQuality\"]  = best[\"quality\"]\n",
    "        df.at[idx, \"ConfidenceScore\"] = round(float(best[\"score\"]), 3)\n",
    "        df.at[idx, \"GeocodeStatus\"]   = \"ok\"\n",
    "    else:\n",
    "        df.at[idx, \"GeocodeSource\"]   = \"no_result\"\n",
    "        df.at[idx, \"GeocodeQuality\"]  = pd.NA\n",
    "        df.at[idx, \"ConfidenceScore\"] = pd.NA\n",
    "        df.at[idx, \"GeocodeStatus\"]   = \"no_result\"\n",
    "\n",
    "# -------------------- PUBLIC API --------------------\n",
    "async def geocode_addresses_free_async(\n",
    "    df: pd.DataFrame,\n",
    "    col_map: Dict[str, str] = None,\n",
    "    max_concurrency: int = MAX_CONCURRENCY\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Free-first, DMV-biased async geocoder.\n",
    "    df must contain: Street, City, State, Zip (or map them via col_map).\n",
    "    \"\"\"\n",
    "    cols = dict(DEFAULT_COLS) if col_map is None else col_map\n",
    "    _normalize_columns(df, cols)\n",
    "\n",
    "    # Only process rows missing coordinates\n",
    "    need_idx = df.index[(df[\"Latitude\"].isna() | df[\"Longitude\"].isna())].tolist()\n",
    "\n",
    "    limits = httpx.Limits(max_connections=max_concurrency, max_keepalive_connections=10)\n",
    "    cache: Dict[str, Optional[Dict]] = {}\n",
    "\n",
    "    async with httpx.AsyncClient(timeout=30, limits=limits, headers={\"Accept\": \"application/json\"}) as client:\n",
    "        sem = asyncio.Semaphore(max_concurrency)\n",
    "\n",
    "        async def worker(i: int):\n",
    "            async with sem:\n",
    "                await _geocode_one_row(i, df, client, cols, cache)\n",
    "\n",
    "        await asyncio.gather(*(worker(i) for i in need_idx))\n",
    "\n",
    "    return df\n",
    "\n",
    "def geocode_addresses_free(df: pd.DataFrame, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Synchronous convenience wrapper for scripts/legacy notebooks.\"\"\"\n",
    "    return asyncio.run(geocode_addresses_free_async(df, **kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27291f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary:\n",
    "# - Maps your existing columns to what the geocoder expects.\n",
    "# - Runs the async geocoder with a DMV bias and free sources (Census → Nominatim).\n",
    "# - Returns `df` with new lat/lon and quality/source columns added.\n",
    "\n",
    "# Tell the geocoder which columns in *your* df contain address parts\n",
    "col_map = {\n",
    "    \"street\": \"Street\",\n",
    "    \"city\":   \"City\",\n",
    "    \"state\":  \"State\",\n",
    "    \"zip\":    \"Zip\",\n",
    "    \"full\":   \"Full Address\"   # This will be created if it doesn't exist\n",
    "}\n",
    "\n",
    "# Run the async geocoding (works in modern Jupyter with top-level `await`)\n",
    "df = await geocode_addresses_free_async(\n",
    "    df,\n",
    "    col_map=col_map,\n",
    "    max_concurrency=6  # OK to adjust; DMV filters & Nominatim limiter are built-in\n",
    ")\n",
    "\n",
    "## Review Results\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97491588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated file\n",
    "print('Saving updated Excel file...')\n",
    "df.to_excel('./data/outbound_geolocated.xlsx', index=False)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426d7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception('End of Functional Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_latitude_and_longitude(df, str_col_names = ['Street', 'City', 'State', 'Zip']):\n",
    "\n",
    "# for col in df.columns:\n",
    "\n",
    "#     if col in str_col_names:\n",
    "#         df[col] = df[col].astype(str)\n",
    "#     else:\n",
    "#         print(f'Skipping column {col} - not included for conversion.')\n",
    "\n",
    "#     # Build full address if not present\n",
    "#     if 'Full Address' not in df.columns:\n",
    "#         df['Full Address'] = (\n",
    "#             df['Street'].fillna('') + ', '\n",
    "#             + df['City'].fillna('') + ', '\n",
    "#             + df['State'].fillna('') + ' '\n",
    "#             + df['Zip'].fillna('')\n",
    "#         )\n",
    "#         df['Full Address'] = df['Full Address'].str.replace(r',\\s*,', ',', regex=True).str.replace(r',\\s*$', '', regex=True)\n",
    "\n",
    "#     # Set up geocoder\n",
    "#     geolocator = Nominatim(user_agent=\"provider_geocoder\")\n",
    "#     geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1, max_retries=3)\n",
    "\n",
    "#     # Add Latitude/Longitude columns if missing\n",
    "#     if 'Latitude' not in df.columns:\n",
    "#         df['Latitude'] = None\n",
    "#     if 'Longitude' not in df.columns:\n",
    "#         df['Longitude'] = None\n",
    "\n",
    "#     # Geocode only missing lat/lon\n",
    "#     for idx, row in df.iterrows():\n",
    "#         if pd.isnull(row['Latitude']) or pd.isnull(row['Longitude']):\n",
    "#             address = row['Full Address']\n",
    "#             try:\n",
    "#                 location = geocode(address, timeout=10)\n",
    "#                 if location:\n",
    "#                     df.at[idx, 'Latitude'] = location.latitude\n",
    "#                     df.at[idx, 'Longitude'] = location.longitude\n",
    "#                 else:\n",
    "#                     print(f\"No result for: {address}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error geocoding {address}: {e}\")\n",
    "#             time.sleep(1)  # Be nice to the API\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summary:\n",
    "# # - Geocodes addresses in a DataFrame using geopy + Nominatim.\n",
    "# # - If the full address fails, retries using only the Street value.\n",
    "# # - Adds Latitude, Longitude, and GeocodeStatus columns.\n",
    "# # - Skips rows that already have coordinates.\n",
    "# # - Caches results to avoid duplicate API calls in the same run.\n",
    "\n",
    "# def get_latitude_and_longitude(\n",
    "#     df: pd.DataFrame,\n",
    "#     str_col_names=('Street', 'City', 'State', 'Zip'),\n",
    "#     address_col='Full Address',\n",
    "#     user_agent='my-firm-provider-geocoder (you@firm.com)',  # <-- replace with your info\n",
    "#     min_delay_seconds=1.0,      # Nominatim-friendly rate limit\n",
    "#     max_retries=2,              # Retries on network/geocoder errors\n",
    "#     error_wait_seconds=2.0,     # Wait between retries on error\n",
    "#     timeout=10,                 # Seconds for each geocode call\n",
    "#     verbose=True\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Geocode a DataFrame of addresses using Nominatim (OpenStreetMap) with a fallback:\n",
    "#     1) Try the full address (Street, City, State, Zip)\n",
    "#     2) If no result, retry using only Street.\n",
    "\n",
    "#     Adds/updates columns:\n",
    "#       - 'Full Address' (if not present)\n",
    "#       - 'Latitude', 'Longitude'\n",
    "#       - 'GeocodeStatus' in {'full_address','street_fallback','no_result','error','skipped_has_coords'}\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     df : pd.DataFrame\n",
    "#         Input DataFrame with at least the columns in str_col_names, or an existing 'Full Address'.\n",
    "#     str_col_names : tuple\n",
    "#         The names of the columns containing Street/City/State/Zip.\n",
    "#     address_col : str\n",
    "#         Name of a prebuilt full-address column (created if missing).\n",
    "#     user_agent : str\n",
    "#         Required by Nominatim ToS; include contact info.\n",
    "#     min_delay_seconds : float\n",
    "#         Minimum delay between geocoding calls (per Nominatim etiquette).\n",
    "#     max_retries : int\n",
    "#         Max retries on transient errors.\n",
    "#     error_wait_seconds : float\n",
    "#         Wait time between retries.\n",
    "#     timeout : int\n",
    "#         Timeout (seconds) for each geocode call.\n",
    "#     verbose : bool\n",
    "#         Print progress messages.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     pd.DataFrame\n",
    "#         The same DataFrame with Latitude/Longitude/GeocodeStatus populated/updated.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ---- 1) Ensure required columns are strings (when present) ----------------\n",
    "#     for col in df.columns:\n",
    "#         if col in str_col_names:\n",
    "#             df[col] = df[col].astype(str).fillna('').str.strip()\n",
    "\n",
    "#     # ---- 2) Build Full Address if not present --------------------------------\n",
    "#     if address_col not in df.columns:\n",
    "#         missing = [c for c in ('Street','City','State','Zip') if c not in df.columns]\n",
    "#         if missing:\n",
    "#             raise ValueError(\n",
    "#                 f\"Missing required columns to build '{address_col}': {missing}\"\n",
    "#             )\n",
    "#         df[address_col] = (\n",
    "#             df['Street'].fillna('').str.strip() + ', ' +\n",
    "#             df['City'].fillna('').str.strip() + ', ' +\n",
    "#             df['State'].fillna('').str.strip() + ' ' +\n",
    "#             df['Zip'].fillna('').str.strip()\n",
    "#         )\n",
    "#         # Clean up accidental extra commas/spaces\n",
    "#         df[address_col] = (\n",
    "#             df[address_col]\n",
    "#               .str.replace(r'\\s+', ' ', regex=True)\n",
    "#               .str.replace(r',\\s*,', ',', regex=True)\n",
    "#               .str.replace(r',\\s*$', '', regex=True)\n",
    "#               .str.strip()\n",
    "#         )\n",
    "\n",
    "#     # ---- 3) Add coordinate/status columns if missing -------------------------\n",
    "#     if 'Latitude' not in df.columns:\n",
    "#         df['Latitude'] = pd.NA\n",
    "#     if 'Longitude' not in df.columns:\n",
    "#         df['Longitude'] = pd.NA\n",
    "#     if 'GeocodeStatus' not in df.columns:\n",
    "#         df['GeocodeStatus'] = pd.NA\n",
    "\n",
    "#     # ---- 4) Set up geocoder with a rate limiter ------------------------------\n",
    "#     geolocator = Nominatim(user_agent=user_agent)\n",
    "#     geocode = RateLimiter(\n",
    "#         geolocator.geocode,\n",
    "#         min_delay_seconds=min_delay_seconds,\n",
    "#         max_retries=max_retries,\n",
    "#         error_wait_seconds=error_wait_seconds,\n",
    "#         swallow_exceptions=True  # we handle errors via None checks\n",
    "#     )\n",
    "\n",
    "#     # ---- 5) Cache to avoid duplicate lookups in this run ---------------------\n",
    "#     # Cache stores: query_text -> (lat, lon) or None if no result\n",
    "#     cache = {}\n",
    "\n",
    "#     # ---- 6) Iterate rows needing geocoding -----------------------------------\n",
    "#     # Only attempt for rows missing either Latitude or Longitude\n",
    "#     mask_need = df['Latitude'].isna() | df['Longitude'].isna()\n",
    "#     rows_to_process = df.loc[mask_need].index.tolist()\n",
    "\n",
    "#     if verbose:\n",
    "#         print(f\"Geocoding {len(rows_to_process)} rows (rows with missing lat/lon).\")\n",
    "\n",
    "#     for idx in rows_to_process:\n",
    "#         street = str(df.at[idx, 'Street']) if 'Street' in df.columns else ''\n",
    "#         full_addr = str(df.at[idx, address_col])\n",
    "\n",
    "#         # If both are empty, skip\n",
    "#         if not (street or full_addr):\n",
    "#             df.at[idx, 'GeocodeStatus'] = 'no_result'\n",
    "#             continue\n",
    "\n",
    "#         # --- Try full address first ------------------------------------------\n",
    "#         lat, lon, status = None, None, None\n",
    "#         query = full_addr.strip()\n",
    "\n",
    "#         # Check cache\n",
    "#         if query in cache:\n",
    "#             cached = cache[query]\n",
    "#             if cached is not None:\n",
    "#                 lat, lon = cached\n",
    "#                 status = 'full_address'\n",
    "#         else:\n",
    "#             # Call geocoder\n",
    "#             try:\n",
    "#                 loc = geocode(query, timeout=timeout, exactly_one=True, addressdetails=False)\n",
    "#                 if loc:\n",
    "#                     lat, lon = loc.latitude, loc.longitude\n",
    "#                     status = 'full_address'\n",
    "#                     cache[query] = (lat, lon)\n",
    "#                 else:\n",
    "#                     cache[query] = None  # no result for this query\n",
    "#             except Exception as _:\n",
    "#                 # swallow_exceptions=True should prevent raising, but keep defensive\n",
    "#                 cache[query] = None\n",
    "\n",
    "#         # --- Fallback: Street-only if needed ---------------------------------\n",
    "#         if lat is None or lon is None:\n",
    "#             street_query = street.strip()\n",
    "#             if street_query:\n",
    "#                 if street_query in cache:\n",
    "#                     cached = cache[street_query]\n",
    "#                     if cached is not None:\n",
    "#                         lat, lon = cached\n",
    "#                         status = 'street_fallback'\n",
    "#                 else:\n",
    "#                     try:\n",
    "#                         loc2 = geocode(street_query, timeout=timeout, exactly_one=True, addressdetails=False)\n",
    "#                         if loc2:\n",
    "#                             lat, lon = loc2.latitude, loc2.longitude\n",
    "#                             status = 'street_fallback'\n",
    "#                             cache[street_query] = (lat, lon)\n",
    "#                         else:\n",
    "#                             cache[street_query] = None\n",
    "#                     except Exception as _:\n",
    "#                         cache[street_query] = None\n",
    "\n",
    "#         # --- Persist results back to df --------------------------------------\n",
    "#         if lat is not None and lon is not None:\n",
    "#             df.at[idx, 'Latitude'] = lat\n",
    "#             df.at[idx, 'Longitude'] = lon\n",
    "#             df.at[idx, 'GeocodeStatus'] = status\n",
    "#         else:\n",
    "#             # Keep as missing if no result\n",
    "#             df.at[idx, 'GeocodeStatus'] = 'no_result'\n",
    "\n",
    "#         # Optional extra delay beyond RateLimiter (usually unnecessary)\n",
    "#         # time.sleep(0.1)\n",
    "\n",
    "#     # Mark rows that were already populated before this function\n",
    "#     already_had = (~mask_need)\n",
    "#     df.loc[already_had & df['GeocodeStatus'].isna(), 'GeocodeStatus'] = 'skipped_has_coords'\n",
    "\n",
    "#     if verbose:\n",
    "#         done = df['GeocodeStatus'].value_counts(dropna=False).to_dict()\n",
    "#         print(f\"Geocoding complete. Status counts: {done}\")\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c915a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_latitude_and_longitude(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec032b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edb459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b277c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c8a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a8f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7860f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f978f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59e8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73880d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
